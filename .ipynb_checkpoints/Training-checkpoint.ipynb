{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7c6948-c8f1-46c9-95ad-958e291ceace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, HeteroData, DataLoader\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.Polypeptide import three_to_index, index_to_one\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set random seed and device\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfae452-2fcb-4486-897d-c87fb5cf291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, Linear, global_mean_pool\n",
    "from torch.utils.data import random_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.Polypeptide import three_to_index, index_to_one\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Helper functions\n",
    "def residue_name_to_idx(res_name_one):\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G',\n",
    "                   'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S',\n",
    "                   'T', 'W', 'Y', 'V']\n",
    "    if res_name_one in amino_acids:\n",
    "        return amino_acids.index(res_name_one)\n",
    "    else:\n",
    "        return len(amino_acids)  # Unknown amino acid\n",
    "\n",
    "def process_protein(pdb_file, threshold=5.0):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('protein', pdb_file)\n",
    "\n",
    "    amino_acids = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if is_aa(residue):\n",
    "                    amino_acids.append(residue)\n",
    "\n",
    "    amino_acid_types = [index_to_one(three_to_index(residue.get_resname())) for residue in amino_acids]\n",
    "    unique_amino_acids = list(set(amino_acid_types))\n",
    "\n",
    "    data = HeteroData()\n",
    "\n",
    "    node_features = {}\n",
    "    node_positions = {}\n",
    "    node_counter = 0\n",
    "\n",
    "    # Initialize node features and positions\n",
    "    for aa_type in unique_amino_acids:\n",
    "        node_features[aa_type] = []\n",
    "        node_positions[aa_type] = []\n",
    "\n",
    "    for idx, (residue, aa_type) in enumerate(zip(amino_acids, amino_acid_types)):\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            pos = ca_atom.get_coord()\n",
    "        except KeyError:\n",
    "            pos = [0.0, 0.0, 0.0]\n",
    "        node_features[aa_type].append([residue_name_to_idx(aa_type)])\n",
    "        node_positions[aa_type].append(pos)\n",
    "        node_counter += 1\n",
    "\n",
    "    for aa_type in unique_amino_acids:\n",
    "        data[aa_type].x = torch.tensor(node_features[aa_type], dtype=torch.float)\n",
    "        data[aa_type].pos = torch.tensor(np.array(node_positions[aa_type]), dtype=torch.float)\n",
    "\n",
    "    # Build edges based on proximity\n",
    "    contact_edge_index = {}\n",
    "    edge_types = set()\n",
    "    reverse_edge_types = set()\n",
    "\n",
    "    # Mapping from global index to local index within node type\n",
    "    global_to_local_idx = {}\n",
    "    current_idx = {aa_type: 0 for aa_type in unique_amino_acids}\n",
    "\n",
    "    for aa_type in amino_acid_types:\n",
    "        global_to_local_idx[aa_type] = {}\n",
    "\n",
    "    for idx, aa_type in enumerate(amino_acid_types):\n",
    "        global_idx = idx\n",
    "        local_idx = current_idx[aa_type]\n",
    "        global_to_local_idx[aa_type][global_idx] = local_idx\n",
    "        current_idx[aa_type] += 1\n",
    "\n",
    "    num_residues = len(amino_acids)\n",
    "    for i in range(num_residues):\n",
    "        residue_i = amino_acids[i]\n",
    "        aa_i = amino_acid_types[i]\n",
    "        try:\n",
    "            ca_i = residue_i['CA']\n",
    "            pos_i = ca_i.get_coord()\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j in range(i + 1, num_residues):  # Ensure j > i to avoid duplicates\n",
    "            residue_j = amino_acids[j]\n",
    "            aa_j = amino_acid_types[j]\n",
    "            try:\n",
    "                ca_j = residue_j['CA']\n",
    "                pos_j = ca_j.get_coord()\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            distance = np.linalg.norm(pos_i - pos_j)\n",
    "            if distance <= threshold:\n",
    "                # Define edge type in consistent order\n",
    "                if aa_i <= aa_j:\n",
    "                    edge_type = (aa_i, 'contact', aa_j)\n",
    "                    src_aa, tgt_aa = aa_i, aa_j\n",
    "                    src_global, tgt_global = i, j\n",
    "                else:\n",
    "                    edge_type = (aa_j, 'contact', aa_i)\n",
    "                    src_aa, tgt_aa = aa_j, aa_i\n",
    "                    src_global, tgt_global = j, i\n",
    "\n",
    "                # Initialize edge list if not present\n",
    "                if edge_type not in contact_edge_index:\n",
    "                    contact_edge_index[edge_type] = []\n",
    "\n",
    "                # Get local indices within their respective node types\n",
    "                src_local = global_to_local_idx[src_aa][src_global]\n",
    "                tgt_local = global_to_local_idx[tgt_aa][tgt_global]\n",
    "\n",
    "                # Append edge\n",
    "                contact_edge_index[edge_type].append([src_local, tgt_local])\n",
    "                edge_types.add(edge_type)\n",
    "\n",
    "    # Assign edges to HeteroData\n",
    "    for edge_type, edges in contact_edge_index.items():\n",
    "        if len(edges) > 0:\n",
    "            # Extract the original source and target types\n",
    "            src_type, relation, tgt_type = edge_type\n",
    "    \n",
    "            # Create reverse edge type\n",
    "            reverse_edge_type = (tgt_type, relation, src_type)\n",
    "            reverse_edge_types.add(reverse_edge_type)\n",
    "    \n",
    "            # Convert original edges to tensor\n",
    "            edge_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "            # Assign original edges to original edge type\n",
    "            data[edge_type].edge_index = edge_tensor\n",
    "    \n",
    "            # Create reverse edges\n",
    "            reverse_edges = [[tgt, src] for src, tgt in edges]\n",
    "            reverse_edge_tensor = torch.tensor(reverse_edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "            # Assign reverse edges to reverse edge type\n",
    "            data[reverse_edge_type].edge_index = reverse_edge_tensor\n",
    "\n",
    "    data.node_types = set(unique_amino_acids)\n",
    "    data.edge_types = edge_types\n",
    "    data.reverse_edge_types = reverse_edge_types\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f2ab8c-1b84-40c8-a1e5-44687096ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "def collect_protein_node_and_edge_types(protein_graphs):\n",
    "    protein_node_types = set()\n",
    "    protein_edge_types = set()\n",
    "    for protein_data in protein_graphs.values():\n",
    "        protein_node_types.update(protein_data.node_types)\n",
    "        protein_edge_types.update(protein_data.edge_types)\n",
    "        protein_edge_types.update(protein_data.reverse_edge_types)\n",
    "    return sorted(protein_node_types), sorted(protein_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da72978b-0b2c-44b2-825c-605e071cead7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split completed.\n",
      "Training set size: 2169734\n",
      "Validation set size: 464943\n",
      "Test set size: 464943\n",
      "Collected molecule node and edge types successfully.\n",
      "Collecting protein node and edge types...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_parquet('filtered_train.parquet')\n",
    "import json\n",
    "\n",
    "# Stratified splitting\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['binds'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nSplit completed.\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Process and store protein graphs\n",
    "protein_graphs = {}\n",
    "protein_pdb_files = {\n",
    "    'BRD4': './BRD4.pdb',\n",
    "    'HSA': './ALB.pdb',\n",
    "    'sEH': './EPH.pdb'\n",
    "}\n",
    "\n",
    "for protein_name, pdb_file in protein_pdb_files.items():\n",
    "    if os.path.exists(pdb_file):\n",
    "        protein_data = process_protein(pdb_file)\n",
    "        protein_graphs[protein_name] = protein_data\n",
    "    else:\n",
    "        print(f\"PDB file {pdb_file} for {protein_name} does not exist.\")\n",
    "\n",
    "# Load the unique atom and edge types from the JSON file\n",
    "with open('unique_atom_and_edge_types.json', 'r') as f:\n",
    "    unique_types = json.load(f)\n",
    "\n",
    "# Extract molecule node and edge types\n",
    "molecule_node_types = unique_types['molecule_node_types']\n",
    "molecule_edge_types = [tuple(edge) for edge in unique_types['molecule_edge_types']]\n",
    "\n",
    "# Now molecule_node_types and molecule_edge_types can be used in your code\n",
    "print(\"Collected molecule node and edge types successfully.\")\n",
    "\n",
    "print(\"Collecting protein node and edge types...\")\n",
    "protein_node_types, protein_edge_types = collect_protein_node_and_edge_types(protein_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65673ed5-853b-4d2c-8300-b4b0211cacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=4):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_K = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_V = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.scale = self.head_dim ** 0.5\n",
    "\n",
    "    def forward(self, query_nodes, key_nodes):\n",
    "        # query_nodes: [N_q, hidden_dim]\n",
    "        # key_nodes: [N_k, hidden_dim]\n",
    "\n",
    "        N_q = query_nodes.size(0)\n",
    "        N_k = key_nodes.size(0)\n",
    "\n",
    "        Q = self.W_Q(query_nodes)  # [N_q, hidden_dim]\n",
    "        K = self.W_K(key_nodes)    # [N_k, hidden_dim]\n",
    "        V = self.W_V(key_nodes)    # [N_k, hidden_dim]\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(N_q, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_q, head_dim]\n",
    "        K = K.view(N_k, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_k, head_dim]\n",
    "        V = V.view(N_k, self.num_heads, self.head_dim).permute(1, 0, 2)  # [num_heads, N_k, head_dim]\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [num_heads, N_q, N_k]\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)                # [num_heads, N_q, N_k]\n",
    "\n",
    "        # Compute attended values\n",
    "        out = torch.matmul(attn_weights, V)  # [num_heads, N_q, head_dim]\n",
    "        out = out.permute(1, 0, 2).contiguous().view(N_q, -1)  # [N_q, hidden_dim]\n",
    "\n",
    "        return out\n",
    "\n",
    "class CrossGraphAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_attention_heads=4):\n",
    "        super(CrossGraphAttentionModel, self).__init__()\n",
    "\n",
    "        # print(molecule_edge_types)\n",
    "\n",
    "        # Molecule GNN Encoder\n",
    "        self.mol_conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in molecule_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.mol_conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in molecule_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Protein GNN Encoder\n",
    "        self.prot_conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in protein_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.prot_conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_dim)\n",
    "            for edge_type in protein_edge_types\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Cross-Attention Layers\n",
    "        self.cross_attn_mol_to_prot = CrossAttentionLayer(hidden_dim, num_attention_heads)\n",
    "        self.cross_attn_prot_to_mol = CrossAttentionLayer(hidden_dim, num_attention_heads)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, mol_data, prot_data):\n",
    "        # Molecule GNN Encoding\n",
    "        x_mol_dict = mol_data.x_dict\n",
    "        edge_index_mol_dict = mol_data.edge_index_dict\n",
    "\n",
    "        x_mol_dict = self.mol_conv1(x_mol_dict, edge_index_mol_dict)\n",
    "        x_mol_dict = {key: F.relu(x) for key, x in x_mol_dict.items()}\n",
    "\n",
    "        x_mol_dict = self.mol_conv2(x_mol_dict, edge_index_mol_dict)\n",
    "        x_mol_dict = {key: F.relu(x) for key, x in x_mol_dict.items()}\n",
    "\n",
    "        # Concatenate molecule node embeddings\n",
    "        mol_node_embeddings = []\n",
    "        for nt in molecule_node_types:\n",
    "            if nt in x_mol_dict:\n",
    "                mol_node_embeddings.append(x_mol_dict[nt])\n",
    "        H_mol = torch.cat(mol_node_embeddings, dim=0)\n",
    "\n",
    "        # Protein GNN Encoding\n",
    "        x_prot_dict = prot_data.x_dict\n",
    "        edge_index_prot_dict = prot_data.edge_index_dict\n",
    "\n",
    "        x_prot_dict = self.prot_conv1(x_prot_dict, edge_index_prot_dict)\n",
    "        x_prot_dict = {key: F.relu(x) for key, x in x_prot_dict.items()}\n",
    "\n",
    "        x_prot_dict = self.prot_conv2(x_prot_dict, edge_index_prot_dict)\n",
    "        x_prot_dict = {key: F.relu(x) for key, x in x_prot_dict.items()}\n",
    "\n",
    "        # Concatenate protein node embeddings\n",
    "        prot_node_embeddings = []\n",
    "        for nt in protein_node_types:\n",
    "            if nt in x_prot_dict:\n",
    "                prot_node_embeddings.append(x_prot_dict[nt])\n",
    "        H_prot = torch.cat(prot_node_embeddings, dim=0)\n",
    "\n",
    "        # Cross-Attention\n",
    "        H_mol_attn = self.cross_attn_mol_to_prot(H_mol, H_prot)\n",
    "        H_prot_attn = self.cross_attn_prot_to_mol(H_prot, H_mol)\n",
    "\n",
    "        # Combine original and attended embeddings\n",
    "        H_mol_combined = H_mol + H_mol_attn\n",
    "        H_prot_combined = H_prot + H_prot_attn\n",
    "\n",
    "        # # Global Pooling\n",
    "        # mol_batch = mol_data.batch if hasattr(mol_data, 'batch') else torch.zeros(H_mol_combined.size(0), dtype=torch.long, device=H_mol_combined.device)\n",
    "        # prot_batch = prot_data.batch if hasattr(prot_data, 'batch') else torch.zeros(H_prot_combined.size(0), dtype=torch.long, device=H_prot_combined.device)\n",
    "\n",
    "        # Global Pooling\n",
    "        # Use batch_dict to get batch information per node type\n",
    "        mol_batches = torch.cat([mol_data.batch_dict[nt] for nt in molecule_node_types if nt in mol_data.batch_dict])\n",
    "        prot_batches = torch.cat([prot_data.batch_dict[nt] for nt in protein_node_types if nt in prot_data.batch_dict])\n",
    "\n",
    "        # z_mol = global_mean_pool(H_mol_combined, mol_batch)\n",
    "        # z_prot = global_mean_pool(H_prot_combined, prot_batch)\n",
    "\n",
    "        z_mol = global_mean_pool(H_mol_combined, mol_batches)\n",
    "        z_prot = global_mean_pool(H_prot_combined, prot_batches)\n",
    "\n",
    "        # Joint Representation\n",
    "        z_joint = torch.cat([z_mol, z_prot], dim=1)\n",
    "\n",
    "        # Prediction\n",
    "        x = F.relu(self.fc1(z_joint))\n",
    "        out = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cd174-623f-4ea8-ad1c-bd902244d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import CombinedDataset, MoleculeDataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CombinedDataset(train_df, protein_graphs)\n",
    "val_dataset = CombinedDataset(val_df, protein_graphs)\n",
    "test_dataset = CombinedDataset(test_df, protein_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed3b69-e231-4ab4-a888-76d9ddcf065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    mol_batch = [item[0] for item in batch if item is not None and item[0] is not None and item[0]['invalid'] is False]\n",
    "    prot_batch = [item[1] for item in batch if item is not None and item[0] is not None and item[0]['invalid'] is False]\n",
    "\n",
    "    mol_batch = Batch.from_data_list(mol_batch)\n",
    "    prot_batch = Batch.from_data_list(prot_batch)\n",
    "\n",
    "    return mol_batch, prot_batch\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = GeoDataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=14)\n",
    "val_loader = GeoDataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=14)\n",
    "test_loader = GeoDataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fa02e-5e66-41be-a558-5e0869fdd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node_and_edge_info(data):\n",
    "    print(\"\\nNode and Edge Information:\")\n",
    "    \n",
    "    # Iterate over all node types and print their counts\n",
    "    for node_type in data.node_types:\n",
    "        if node_type == 'smolecule':  # Ignore smolecule\n",
    "            continue\n",
    "        \n",
    "        num_nodes = data[node_type].x.size(0)\n",
    "        print(f\"Node type: {node_type}, Number of nodes: {num_nodes}\")\n",
    "    \n",
    "    # Iterate over all edge types and print relevant information\n",
    "    for edge_type, edge_index in data.edge_index_dict.items():\n",
    "        if 'smolecule' in edge_type:  # Ignore any edges involving smolecule\n",
    "            continue\n",
    "        \n",
    "        src_type, _, tgt_type = edge_type\n",
    "        max_src_idx = edge_index[0].max().item()\n",
    "        max_tgt_idx = edge_index[1].max().item()\n",
    "\n",
    "        num_src_nodes = data[src_type].x.size(0)\n",
    "        num_tgt_nodes = data[tgt_type].x.size(0)\n",
    "\n",
    "        print(f\"Edge type: {edge_type}, Edge index shape: {edge_index.shape}\")\n",
    "        print(f\"Max index in edge_index: src = {max_src_idx}, tgt = {max_tgt_idx}\")\n",
    "        print(f\"Num src nodes ({src_type}): {num_src_nodes}, Num tgt nodes ({tgt_type}): {num_tgt_nodes}\")\n",
    "\n",
    "        # Validation check to identify if there are out-of-bound indices\n",
    "        if max_src_idx >= num_src_nodes or max_tgt_idx >= num_tgt_nodes:\n",
    "            print(f\"Warning: Edge indices out of bounds for edge type {edge_type}\")\n",
    "            print(f\"  Max src index: {max_src_idx} (Num src nodes: {num_src_nodes})\")\n",
    "            print(f\"  Max tgt index: {max_tgt_idx} (Num tgt nodes: {num_tgt_nodes})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6a80f-e6f7-41c6-8369-0d0830e2ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"You are using `torch.load` with `weights_only=False`\")\n",
    "\n",
    "hidden_dim = 64\n",
    "num_attention_heads = 4\n",
    "\n",
    "model = CrossGraphAttentionModel(\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_attention_heads=num_attention_heads\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mol_data, prot_data in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        mol_data = mol_data.to(device)\n",
    "        prot_data = prot_data.to(device)\n",
    "        # print_node_and_edge_info(mol_data)\n",
    "        out = model(mol_data, prot_data)\n",
    "        y = mol_data['smolecule'].y.to(device)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(loader, mode='Validation'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mol_data, prot_data in tqdm(loader, desc=mode):\n",
    "            mol_data = mol_data.to(device)\n",
    "            prot_data = prot_data.to(device)\n",
    "            out = model(mol_data, prot_data)\n",
    "            y = mol_data['smolecule'].y.to(device)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss = evaluate(val_loader, mode='Validation')\n",
    "    print(f'Epoch: {epoch:02d}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'cross_graph_attention_model.pth')\n",
    "\n",
    "# Prediction and evaluation on test data\n",
    "def predict(loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for mol_data, prot_data in tqdm(loader, desc=\"Testing\"):\n",
    "            mol_data = mol_data.to(device)\n",
    "            prot_data = prot_data.to(device)\n",
    "            out = model(mol_data, prot_data)\n",
    "            predictions.extend(out.cpu().numpy())\n",
    "            true_labels.extend(mol_data['smolecule'].y.cpu().numpy())\n",
    "    return predictions, true_labels\n",
    "\n",
    "test_predictions, test_true = predict(test_loader)\n",
    "\n",
    "# Apply a threshold to obtain binary predictions\n",
    "threshold = 0.5\n",
    "test_pred_binary = [1 if p >= threshold else 0 for p in test_predictions]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(test_true, test_pred_binary)\n",
    "roc_auc = roc_auc_score(test_true, test_predictions)\n",
    "precision = precision_score(test_true, test_pred_binary)\n",
    "recall = recall_score(test_true, test_pred_binary)\n",
    "f1 = f1_score(test_true, test_pred_binary)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e97d5bf-8c12-457e-95d8-506b7bfd001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   3%|█████▎                                                                                                                                                                | 3326/104681 [11:25<5:41:39,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping index 51 due to error: Bad Conformer Id\n",
      "Skipping index 51 due to error: Bad Conformer Id\n",
      "Skipping index 51 due to error: Bad Conformer Id\n",
      "Skipping index 54 due to error: Bad Conformer Id\n",
      "Skipping index 54 due to error: Bad Conformer Id\n",
      "Skipping index 54 due to error: Bad Conformer Id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   3%|█████▎                                                                                                                                                                | 3326/104681 [11:26<5:48:46,  4.84it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 13 but got size 16 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m                     writer\u001b[38;5;241m.\u001b[39mwriterow({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(id_val), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pred)})\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions have been written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m external_predict(external_test_loader, output_csv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmissions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mexternal_predict\u001b[0;34m(loader, output_csv_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m mol_data \u001b[38;5;241m=\u001b[39m mol_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m prot_data \u001b[38;5;241m=\u001b[39m prot_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m out \u001b[38;5;241m=\u001b[39m model(mol_data, prot_data)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Extract the 'id' from mol_data['smolecule']\u001b[39;00m\n\u001b[1;32m     41\u001b[0m ids \u001b[38;5;241m=\u001b[39m mol_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmolecule\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 138\u001b[0m, in \u001b[0;36mCrossGraphAttentionModel.forward\u001b[0;34m(self, mol_data, prot_data)\u001b[0m\n\u001b[1;32m    135\u001b[0m z_prot \u001b[38;5;241m=\u001b[39m global_mean_pool(H_prot_combined, prot_batches)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Joint Representation\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m z_joint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z_mol, z_prot], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(z_joint))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 13 but got size 16 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "import warnings\n",
    "from datasets import CombinedDataset, MoleculeDataset\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import csv\n",
    "\n",
    "external_test_df = pd.read_parquet('cleaned_test.parquet')\n",
    "# Specify the device\n",
    "device = torch.device('cpu')  # Use 'cuda' if you have a GPU available\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('cross_graph_attention_model.pth', map_location=device, weights_only=True)\n",
    "model = CrossGraphAttentionModel(hidden_dim=64, num_attention_heads=4)\n",
    "state_dict = torch.load('cross_graph_attention_model.pth', map_location=device)\n",
    "model.to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "external_test_dataset = CombinedDataset(external_test_df, protein_graphs)\n",
    "\n",
    "external_test_loader = GeoDataLoader(external_test_dataset, batch_size=16, shuffle=False, num_workers=14)\n",
    "\n",
    "def external_predict(loader, output_csv_path):\n",
    "    with open(output_csv_path, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['id', 'binds']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for mol_data, prot_data in tqdm(loader, desc=\"Predicting\"):\n",
    "                node_types, _ = mol_data.metadata()\n",
    "                if node_types[0] == 'dummy_data':\n",
    "                    continue\n",
    "\n",
    "                mol_data = mol_data.to(device)\n",
    "                prot_data = prot_data.to(device)\n",
    "                out = model(mol_data, prot_data)\n",
    "\n",
    "                # Extract the 'id' from mol_data['smolecule']\n",
    "                ids = mol_data['smolecule'].id.cpu().numpy()\n",
    "                predictions = out.cpu().numpy()\n",
    "\n",
    "                # Write the results directly to CSV\n",
    "                for id_val, pred in zip(ids, predictions):\n",
    "                    writer.writerow({'id': int(id_val), 'binds': float(pred)})\n",
    "\n",
    "    print(f\"Predictions have been written to {output_csv_path}\")\n",
    "\n",
    "external_predict(external_test_loader, output_csv_path='submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06d668-3a56-4ec1-960d-6363f8aab9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
