{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b952cb-00a1-4e51-bbd5-02fcc0baaae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles', 'molecule_smiles', 'protein_name']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('test.parquet')\n",
    "print(parquet_file.schema.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5e258-03e7-4745-b09d-783c1ed20fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e723c9-0aed-4669-89a4-a1f45bcf0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to input and output Parquet files\n",
    "input_file = 'train.parquet'\n",
    "output_file = 'filtered_train.parquet'\n",
    "\n",
    "# Remove output file if it exists\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Open the Parquet file\n",
    "pf = pq.ParquetFile(input_file)\n",
    "\n",
    "# Get total number of row groups (batches)\n",
    "total_row_groups = pf.num_row_groups\n",
    "\n",
    "# First Pass: Build mapping of molecule_smiles to the set of proteins it binds to\n",
    "print(\"First Pass: Building molecule to proteins mapping...\")\n",
    "\n",
    "# Initialize a dictionary to store mappings\n",
    "molecule_binds = defaultdict(set)\n",
    "all_proteins = set()\n",
    "\n",
    "for rg in tqdm(range(total_row_groups), desc=\"Processing Batches\"):\n",
    "    # Read a row group with necessary columns\n",
    "    batch = pf.read_row_group(rg, columns=['molecule_smiles', 'protein_name', 'binds'])\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Update all_proteins set\n",
    "    all_proteins.update(df['protein_name'].unique())\n",
    "    \n",
    "    # Filter rows where 'binds' == 1\n",
    "    df_binds_1 = df[df['binds'] == 1]\n",
    "    \n",
    "    # Update molecule_binds mapping\n",
    "    for idx, row in df_binds_1.iterrows():\n",
    "        molecule = row['molecule_smiles']\n",
    "        protein = row['protein_name']\n",
    "        molecule_binds[molecule].add(protein)\n",
    "    \n",
    "    # Clear variables to free memory\n",
    "    del df, df_binds_1, batch\n",
    "\n",
    "# Convert all_proteins to a list\n",
    "all_proteins = list(all_proteins)\n",
    "\n",
    "# Second Pass: Process data and write to output\n",
    "print(\"Second Pass: Filtering dataset and writing to new Parquet file...\")\n",
    "\n",
    "# Initialize Parquet writer\n",
    "writer = None\n",
    "\n",
    "for rg in tqdm(range(total_row_groups), desc=\"Processing Batches\"):\n",
    "    # Read the row group\n",
    "    batch = pf.read_row_group(rg)\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Filter molecules that have at least one binds == 1\n",
    "    df = df[df['molecule_smiles'].isin(molecule_binds.keys())]\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Prepare to select rows to include\n",
    "        rows_to_include = []\n",
    "        \n",
    "        # Process each molecule in the batch\n",
    "        for molecule, group in df.groupby('molecule_smiles'):\n",
    "            binds_1_proteins = molecule_binds[molecule]\n",
    "            num_binds_1 = len(binds_1_proteins)\n",
    "            \n",
    "            if num_binds_1 > 1:\n",
    "                # Include all rows for this molecule\n",
    "                rows_to_include.append(group)\n",
    "            elif num_binds_1 == 1:\n",
    "                # Include the positive binding row\n",
    "                positive_row = group[group['binds'] == 1]\n",
    "                \n",
    "                # Include one negative binding row\n",
    "                unbound_proteins = set(all_proteins) - binds_1_proteins\n",
    "                # Select one unbound protein\n",
    "                unbound_protein = unbound_proteins.pop()\n",
    "                negative_row = group[(group['protein_name'] == unbound_protein) & (group['binds'] == 0)]\n",
    "                \n",
    "                # If negative_row is empty, skp it\n",
    "                if  not negative_row.empty:\n",
    "                    # Append the positive and negative rows\n",
    "                    rows_to_include.append(pd.concat([positive_row, negative_row]))\n",
    "\n",
    "        \n",
    "        if rows_to_include:\n",
    "            filtered_df = pd.concat(rows_to_include)\n",
    "            \n",
    "            # Convert to PyArrow Table\n",
    "            table = pa.Table.from_pandas(filtered_df)\n",
    "            \n",
    "            # Initialize the Parquet writer if not already done\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(output_file, table.schema)\n",
    "            \n",
    "            # Write the table to the Parquet file\n",
    "            writer.write_table(table)\n",
    "            \n",
    "            # Clear variables to free memory\n",
    "            del table, filtered_df\n",
    "    \n",
    "    # Clear variables to free memory\n",
    "    del df, batch\n",
    "\n",
    "# Close the Parquet writer\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "print(\"Filtering completed. Filtered dataset saved to 'filtered_train.parquet'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978c650-5392-4775-80c8-cf190519c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('filtered_train.parquet')\n",
    "test_df = dd.read_parquet('cleaned_test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of training rows: {total_rows}\")\n",
    "\n",
    "# Total number of rows in test dataset\n",
    "test_total_rows = test_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of test rows: {test_total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a519cd-3f2c-4eb3-b5f8-930749f78136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows in parallel: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1674896/1674896 [1:59:01<00:00, 234.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from joblib import Parallel, delayed\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your original dataframes\n",
    "train_df = pd.read_parquet('filtered_train.parquet')\n",
    "test_df = pd.read_parquet('test.parquet')\n",
    "\n",
    "# Define a function to process each row\n",
    "def process_row(row):\n",
    "    try:\n",
    "        smiles = row['molecule_smiles']\n",
    "        binds = 0\n",
    "\n",
    "        if 'binds' in row:\n",
    "            binds = row['binds']\n",
    "        protein_name = row['protein_name']\n",
    "\n",
    "        # Convert SMILES to molecular graph\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None  # Skip invalid SMILES\n",
    "\n",
    "        # Remove atoms and process the molecule\n",
    "        atoms_to_remove = [atom.GetIdx() for atom in mol.GetAtoms() if atom.GetSymbol() == 'Dy']\n",
    "        mol = Chem.EditableMol(mol)\n",
    "        for idx in sorted(atoms_to_remove, reverse=True):\n",
    "            mol.RemoveAtom(idx)\n",
    "        mol = mol.GetMol()\n",
    "\n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # Embed molecule\n",
    "        AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "\n",
    "        # Check if conformer is valid\n",
    "        try:\n",
    "            conformer = mol.GetConformer()\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "        if conformer is None:\n",
    "            return None\n",
    "\n",
    "        # If the row passes all checks, return it\n",
    "        return row\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Function to filter and save cleaned data\n",
    "def filter_and_save(df, output_file):\n",
    "    # Process the dataframe rows in parallel with progress tracking\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_row)(row) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows in parallel\")\n",
    "    )\n",
    "\n",
    "    # Filter out None rows and create a new DataFrame from valid rows\n",
    "    valid_rows = [result for result in results if result is not None]\n",
    "    valid_df = pd.DataFrame(valid_rows)\n",
    "\n",
    "    # Save the cleaned data to a new parquet file\n",
    "    table = pa.Table.from_pandas(valid_df)\n",
    "    pq.write_table(table, output_file)\n",
    "\n",
    "# Filter and save train and test datasets\n",
    "# filter_and_save(train_df, 'cleaned_train.parquet')\n",
    "filter_and_save(test_df, 'cleaned_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03490864-9759-4e29-a691-48a36a24ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 3099349\n",
      "Number of positive bindings: 1589767\n",
      "Number of negative bindings: 1509582\n",
      "Percentage of positive bindings: 51.29%\n",
      "Percentage of negative bindings: 48.71%\n",
      "Total unique proteins in train dataset: 3\n",
      "Total unique proteins in test dataset: 3\n",
      "Total unique proteins in both datasets: 3\n",
      "Unique proteins in both datasets: <ArrowStringArray>\n",
      "['BRD4', 'sEH', 'HSA']\n",
      "Length: 3, dtype: string\n",
      "Total unique building blocks (train and test): 2110\n",
      "Total unique small molecules (train and test): 2387581\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('cleaned_train.parquet')\n",
    "test_df = dd.read_parquet('cleaned_test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a31d9b-8d64-4502-b758-30cc1cec76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate unique atom and edge types from a dataframe\n",
    "def collect_unique_atom_and_edge_types(df):\n",
    "    # Process the dataframe rows in parallel with progress tracking\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_row_for_types)(row) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting atom and edge types\")\n",
    "    )\n",
    "\n",
    "    # Collect unique atom and edge types\n",
    "    molecule_node_types = set()\n",
    "    molecule_edge_types = set()\n",
    "    for node_types, edge_types in results:\n",
    "        molecule_node_types.update(node_types)\n",
    "        molecule_edge_types.update(edge_types)\n",
    "\n",
    "    return molecule_node_types, molecule_edge_types\n",
    "\n",
    "# Define a function to process each row for atom and edge type collection\n",
    "def process_row_for_types(row):\n",
    "    try:\n",
    "        smiles = row['molecule_smiles']\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return set(), set()  # Skip invalid SMILES\n",
    "\n",
    "        # Remove atoms and process the molecule\n",
    "        atoms_to_remove = [atom.GetIdx() for atom in mol.GetAtoms() if atom.GetSymbol() == 'Dy']\n",
    "        mol = Chem.EditableMol(mol)\n",
    "        for idx in sorted(atoms_to_remove, reverse=True):\n",
    "            mol.RemoveAtom(idx)\n",
    "        mol = mol.GetMol()\n",
    "\n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # Collect atom and edge types\n",
    "        atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        unique_atom_types = set(atom_types)\n",
    "        edge_types = set()\n",
    "\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            atype_i = atom_types[i]\n",
    "            atype_j = atom_types[j]\n",
    "            edge_types.add((atype_i, 'bond', atype_j))\n",
    "            edge_types.add((atype_j, 'bond', atype_i))\n",
    "\n",
    "        return unique_atom_types, edge_types\n",
    "\n",
    "    except Exception as e:\n",
    "        return set(), set()\n",
    "\n",
    "# Load the cleaned data\n",
    "cleaned_train_df = pd.read_parquet('cleaned_train.parquet')\n",
    "cleaned_test_df = pd.read_parquet('test.parquet')\n",
    "\n",
    "# Collect unique atom and edge types from both train and test datasets\n",
    "print(\"Collecting unique atom and edge types from train dataset...\")\n",
    "train_node_types, train_edge_types = collect_unique_atom_and_edge_types(cleaned_train_df)\n",
    "\n",
    "print(\"Collecting unique atom and edge types from test dataset...\")\n",
    "test_node_types, test_edge_types = collect_unique_atom_and_edge_types(cleaned_test_df)\n",
    "\n",
    "# Combine the unique types from both datasets\n",
    "combined_node_types = train_node_types.union(test_node_types)\n",
    "combined_edge_types = train_edge_types.union(test_edge_types)\n",
    "\n",
    "# Save the combined unique atom and edge types to a JSON file\n",
    "unique_types = {\n",
    "    'molecule_node_types': sorted(list(combined_node_types)),\n",
    "    'molecule_edge_types': sorted([list(edge_type) for edge_type in combined_edge_types])\n",
    "}\n",
    "with open('unique_atom_and_edge_types.json', 'w') as f:\n",
    "    json.dump(unique_types, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f880429-749d-4592-ac36-93113be98b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
