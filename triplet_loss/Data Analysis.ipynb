{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc08147-e0db-4a7e-b31a-630db155809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('train.parquet')\n",
    "print(parquet_file.schema.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b78321-f080-459e-a494-2c842399c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0b1ae-6b2f-471f-bf90-23bf1f03ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of training rows: {total_rows}\")\n",
    "\n",
    "# Total number of rows in test dataset\n",
    "test_total_rows = test_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of test rows: {test_total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b010c-993f-4c36-a96c-ca1de2f96181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import cycle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def check_smiles(smiles):\n",
    "    try:\n",
    "        # Convert SMILES to molecular graph\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return False  # Skip invalid SMILES\n",
    "\n",
    "        # Remove atoms and process the molecule\n",
    "        atoms_to_remove = [atom.GetIdx() for atom in mol.GetAtoms() if atom.GetSymbol() == 'Dy']\n",
    "        mol = Chem.EditableMol(mol)\n",
    "        for idx in sorted(atoms_to_remove, reverse=True):\n",
    "            mol.RemoveAtom(idx)\n",
    "        mol = mol.GetMol()\n",
    "\n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # Embed molecule\n",
    "        AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "\n",
    "        # Check if conformer is valid\n",
    "        try:\n",
    "            conformer = mol.GetConformer()\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "        if conformer is None:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Constants\n",
    "proteins = ['HSA', 'sEH', 'BRD4']\n",
    "\n",
    "\n",
    "\n",
    "def process_chunk(chunk, output_dir, chunk_id):\n",
    "    \"\"\"Process a chunk of the dataframe and save binds/non-binds to separate temporary files for each protein.\"\"\"\n",
    "    for protein in proteins:\n",
    "        # Filter based on protein\n",
    "        df_protein = chunk[chunk['protein_name'] == protein]\n",
    "        \n",
    "        # Separate into binds and non-binds\n",
    "        df_binds = df_protein[df_protein['binds'] == 1]\n",
    "        df_non_binds = df_protein[df_protein['binds'] == 0]\n",
    "\n",
    "        # Write binds and non-binds to separate temporary files with unique names\n",
    "        if not df_binds.empty:\n",
    "            binds_file = f\"{output_dir}/{protein}_binds_chunk_{chunk_id}.parquet\"\n",
    "            df_binds.to_parquet(binds_file, engine='pyarrow', compression='snappy')\n",
    "\n",
    "        if not df_non_binds.empty:\n",
    "            non_binds_file = f\"{output_dir}/{protein}_non_binds_chunk_{chunk_id}.parquet\"\n",
    "            df_non_binds.to_parquet(non_binds_file, engine='pyarrow', compression='snappy')\n",
    "\n",
    "def merge_and_sample_non_binds(output_dir, protein):\n",
    "    \"\"\"Merge temporary Parquet files for binds/non-binds and sample two non-binds for each bind.\"\"\"\n",
    "    binds_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith(f\"{protein}_binds_chunk_\")]\n",
    "    non_binds_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith(f\"{protein}_non_binds_chunk_\")]\n",
    "\n",
    "    if not binds_files or not non_binds_files:\n",
    "        return None\n",
    "\n",
    "    # Create iterators for binds and non-binds\n",
    "    binds_data = pd.concat(pd.read_parquet(file, engine='pyarrow') for file in binds_files)\n",
    "    binds_iter = cycle(binds_data.iterrows())  # Create a cyclic iterator\n",
    "\n",
    "    # Open output file\n",
    "    output_file = f\"{output_dir}/{protein}_cleaned.parquet\"\n",
    "    writer = None\n",
    "\n",
    "    result_rows = []\n",
    "    bind_count = 0\n",
    "    max_binds = len(binds_data) * 3  # Process each bind 3 times\n",
    "\n",
    "    non_binds_file_iter = iter(non_binds_files)\n",
    "    current_non_binds = pd.read_parquet(next(non_binds_file_iter), engine='pyarrow')\n",
    "    non_binds_row_iter = iter(current_non_binds.iterrows())\n",
    "\n",
    "    while bind_count < max_binds:\n",
    "        _, bind_row = next(binds_iter)\n",
    "        \n",
    "        # Check bind SMILES\n",
    "        if not check_smiles(bind_row['molecule_smiles']):\n",
    "            continue  # Skip this bind row and move to the next\n",
    "\n",
    "        non_binds = []\n",
    "        while len(non_binds) < 2:\n",
    "            try:\n",
    "                _, non_bind = next(non_binds_row_iter)\n",
    "            except StopIteration:\n",
    "                # If we've exhausted the current file, move to the next one\n",
    "                try:\n",
    "                    current_non_binds = pd.read_parquet(next(non_binds_file_iter), engine='pyarrow')\n",
    "                    non_binds_row_iter = iter(current_non_binds.iterrows())\n",
    "                    continue # We found more data to go through\n",
    "                except StopIteration:\n",
    "                    # If we've gone through all files, break the loop\n",
    "                    print(f\"Ran out of non-binds after processing {bind_count} binds\")\n",
    "                    break\n",
    "\n",
    "            # Check non-bind SMILES\n",
    "            if check_smiles(non_bind['molecule_smiles']):\n",
    "                non_binds.append(non_bind)\n",
    "\n",
    "        # If we couldn't find 2 valid non-binds, end processing\n",
    "        if len(non_binds) < 2:\n",
    "            break\n",
    "\n",
    "        # Append a row to the result\n",
    "        result_rows.append({\n",
    "            'id': bind_row['id'],\n",
    "            'smiles_binds': bind_row['molecule_smiles'],\n",
    "            'smiles_non_binds_1': non_binds[0]['molecule_smiles'],\n",
    "            'smiles_non_binds_2': non_binds[1]['molecule_smiles'],\n",
    "            'protein_name': bind_row['protein_name']\n",
    "        })\n",
    "\n",
    "        bind_count += 1\n",
    "\n",
    "        # Write to the output file in batches\n",
    "        if len(result_rows) >= 10000:\n",
    "            result_df = pd.DataFrame(result_rows)\n",
    "            if writer is None:\n",
    "                writer = pa.parquet.ParquetWriter(output_file, schema=pa.Table.from_pandas(result_df).schema)\n",
    "            writer.write_table(pa.Table.from_pandas(result_df))\n",
    "            result_rows = []\n",
    "\n",
    "    # Write any remaining rows\n",
    "    if result_rows:\n",
    "        result_df = pd.DataFrame(result_rows)\n",
    "        if writer is None:\n",
    "            writer = pa.parquet.ParquetWriter(output_file, schema=pa.Table.from_pandas(result_df).schema)\n",
    "        writer.write_table(pa.Table.from_pandas(result_df))\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    # Clean up the intermediate files\n",
    "    for f in binds_files + non_binds_files:\n",
    "        os.remove(f)\n",
    "\n",
    "def process_large_parquet(input_file, output_dir):\n",
    "    \"\"\"Process the large dataset in parallel chunks and save to temporary files.\"\"\"\n",
    "    # Open parquet file\n",
    "    parquet_file = pq.ParquetFile(input_file)\n",
    "    \n",
    "    # Process each row group (chunk) in parallel\n",
    "    tasks = []\n",
    "    with tqdm(total=parquet_file.num_row_groups, dynamic_ncols=True) as pbar:\n",
    "        for i in range(parquet_file.num_row_groups):\n",
    "            # Read a chunk of the parquet file\n",
    "            chunk = parquet_file.read_row_group(i)\n",
    "\n",
    "            # Append the task to the list\n",
    "            tasks.append(delayed(process_chunk)(chunk, output_dir, i))\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Execute the tasks in parallel\n",
    "    Parallel(n_jobs=-1)(tasks)\n",
    "\n",
    "    # Merge and sample non-binds for each protein\n",
    "    for protein in proteins:\n",
    "        merge_and_sample_non_binds(output_dir, protein)\n",
    "\n",
    "    # Merge the final cleaned data for all proteins into one file\n",
    "    cleaned_files = [f\"{output_dir}/{protein}_cleaned.parquet\" for protein in proteins]\n",
    "    df_final = pd.concat([pd.read_parquet(f) for f in cleaned_files])\n",
    "    df_final.to_parquet(f\"{output_dir}/cleaned_train.parquet\", engine='pyarrow', compression='snappy')\n",
    "\n",
    "    # Clean up intermediate files\n",
    "    for f in cleaned_files:\n",
    "        os.remove(f)\n",
    "\n",
    "# Define the input file and output directory\n",
    "input_parquet = \"train.parquet\"\n",
    "output_directory = \".\"\n",
    "\n",
    "# Run the parallel processing and merging\n",
    "process_large_parquet(input_parquet, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
