{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc08147-e0db-4a7e-b31a-630db155809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles', 'molecule_smiles', 'protein_name', 'binds']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('train.parquet')\n",
    "print(parquet_file.schema.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f0b1ae-6b2f-471f-bf90-23bf1f03ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training rows: 295246830\n",
      "Total number of test rows: 1674896\n",
      "Number of positive bindings: 1589906\n",
      "Number of negative bindings: 293656924\n",
      "Percentage of positive bindings: 0.54%\n",
      "Percentage of negative bindings: 99.46%\n",
      "Total unique proteins in train dataset: 3\n",
      "Total unique proteins in test dataset: 3\n",
      "Total unique proteins in both datasets: 3\n",
      "Unique proteins in both datasets: <ArrowStringArray>\n",
      "['HSA', 'sEH', 'BRD4']\n",
      "Length: 3, dtype: string\n",
      "Total unique building blocks (train and test): 2110\n",
      "Total unique small molecules (train and test): 99293632\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of training rows: {total_rows}\")\n",
    "\n",
    "# Total number of rows in test dataset\n",
    "test_total_rows = test_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of test rows: {test_total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6afec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unique atom and edge types from train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting atom and edge types: 100%|██████████| 1589770/1589770 [01:29<00:00, 17704.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unique atom and edge types from test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting atom and edge types: 100%|██████████| 1674896/1674896 [01:32<00:00, 18067.89it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m\n\u001b[1;32m     72\u001b[0m unique_types \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmolecule_node_types\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(combined_node_types)),\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmolecule_edge_types\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msorted\u001b[39m([\u001b[38;5;28mlist\u001b[39m(edge_type) \u001b[38;5;28;01mfor\u001b[39;00m edge_type \u001b[38;5;129;01min\u001b[39;00m combined_edge_types])\n\u001b[1;32m     75\u001b[0m }\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_atom_and_edge_types.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 77\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(unique_types, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Define a function to calculate unique atom and edge types from a dataframe\n",
    "def collect_unique_atom_and_edge_types(df):\n",
    "    # Process the dataframe rows in parallel with progress tracking\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_row_for_types)(row) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting atom and edge types\")\n",
    "    )\n",
    "\n",
    "    # Collect unique atom and edge types\n",
    "    molecule_node_types = set()\n",
    "    molecule_edge_types = set()\n",
    "    for node_types, edge_types in results:\n",
    "        molecule_node_types.update(node_types)\n",
    "        molecule_edge_types.update(edge_types)\n",
    "\n",
    "    return molecule_node_types, molecule_edge_types\n",
    "\n",
    "# Define a function to process each row for atom and edge type collection\n",
    "def process_row_for_types(row):\n",
    "    try:\n",
    "        smiles = row['molecule_smiles']\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return set(), set()  # Skip invalid SMILES\n",
    "\n",
    "        # Remove atoms and process the molecule\n",
    "        atoms_to_remove = [atom.GetIdx() for atom in mol.GetAtoms() if atom.GetSymbol() == 'Dy']\n",
    "        mol = Chem.EditableMol(mol)\n",
    "        for idx in sorted(atoms_to_remove, reverse=True):\n",
    "            mol.RemoveAtom(idx)\n",
    "        mol = mol.GetMol()\n",
    "\n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # Collect atom and edge types\n",
    "        atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        unique_atom_types = set(atom_types)\n",
    "        edge_types = set()\n",
    "\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            atype_i = atom_types[i]\n",
    "            atype_j = atom_types[j]\n",
    "            edge_types.add((atype_i, 'bond', atype_j))\n",
    "            edge_types.add((atype_j, 'bond', atype_i))\n",
    "\n",
    "        return unique_atom_types, edge_types\n",
    "\n",
    "    except Exception as e:\n",
    "        return set(), set()\n",
    "\n",
    "# Load the cleaned data\n",
    "cleaned_train_df = pd.read_parquet('cleaned_train_unique.parquet')\n",
    "cleaned_test_df = pd.read_parquet('test.parquet')\n",
    "\n",
    "# Collect unique atom and edge types from both train and test datasets\n",
    "print(\"Collecting unique atom and edge types from train dataset...\")\n",
    "train_node_types, train_edge_types = collect_unique_atom_and_edge_types(cleaned_train_df)\n",
    "\n",
    "print(\"Collecting unique atom and edge types from test dataset...\")\n",
    "test_node_types, test_edge_types = collect_unique_atom_and_edge_types(cleaned_test_df)\n",
    "\n",
    "# Combine the unique types from both datasets\n",
    "combined_node_types = train_node_types.union(test_node_types)\n",
    "combined_edge_types = train_edge_types.union(test_edge_types)\n",
    "\n",
    "# Save the combined unique atom and edge types to a JSON file\n",
    "unique_types = {\n",
    "    'molecule_node_types': sorted(list(combined_node_types)),\n",
    "    'molecule_edge_types': sorted([list(edge_type) for edge_type in combined_edge_types])\n",
    "}\n",
    "with open('unique_atom_and_edge_types.json', 'w') as f:\n",
    "    json.dump(unique_types, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
