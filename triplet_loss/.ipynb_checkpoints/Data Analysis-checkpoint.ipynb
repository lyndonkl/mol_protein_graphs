{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc08147-e0db-4a7e-b31a-630db155809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles', 'molecule_smiles', 'protein_name', 'binds']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('train.parquet')\n",
    "print(parquet_file.schema.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0b1ae-6b2f-471f-bf90-23bf1f03ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training rows: 295246830\n",
      "Total number of test rows: 1674896\n",
      "Number of positive bindings: 1589906\n",
      "Number of negative bindings: 293656924\n",
      "Percentage of positive bindings: 0.54%\n",
      "Percentage of negative bindings: 99.46%\n",
      "Total unique proteins in train dataset: 3\n",
      "Total unique proteins in test dataset: 3\n",
      "Total unique proteins in both datasets: 3\n",
      "Unique proteins in both datasets: <ArrowStringArray>\n",
      "['HSA', 'sEH', 'BRD4']\n",
      "Length: 3, dtype: string\n",
      "Total unique building blocks (train and test): 2110\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of training rows: {total_rows}\")\n",
    "\n",
    "# Total number of rows in test dataset\n",
    "test_total_rows = test_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of test rows: {test_total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b010c-993f-4c36-a96c-ca1de2f96181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from itertools import cycle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import random\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def check_smiles(smiles):\n",
    "    try:\n",
    "        # Convert SMILES to molecular graph\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return False  # Skip invalid SMILES\n",
    "\n",
    "        # Remove atoms and process the molecule\n",
    "        atoms_to_remove = [atom.GetIdx() for atom in mol.GetAtoms() if atom.GetSymbol() == 'Dy']\n",
    "        mol = Chem.EditableMol(mol)\n",
    "        for idx in sorted(atoms_to_remove, reverse=True):\n",
    "            mol.RemoveAtom(idx)\n",
    "        mol = mol.GetMol()\n",
    "\n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # Embed molecule\n",
    "        AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "\n",
    "        # Check if conformer is valid\n",
    "        try:\n",
    "            conformer = mol.GetConformer()\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "        if conformer is None:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Constants\n",
    "proteins = ['HSA', 'sEH', 'BRD4']\n",
    "\n",
    "def process_chunk(chunk, output_dir, chunk_id):\n",
    "    \"\"\"Process a chunk of the dataframe and save binds/non-binds to separate temporary files for each protein.\"\"\"\n",
    "    logger.info(f\"Processing chunk {chunk_id}\")\n",
    "    for protein in proteins:\n",
    "        # Filter based on protein\n",
    "        df_protein = chunk[chunk['protein_name'] == protein]\n",
    "        \n",
    "        # Separate into binds and non-binds\n",
    "        df_binds = df_protein[df_protein['binds'] == 1]\n",
    "        df_non_binds = df_protein[df_protein['binds'] == 0]\n",
    "\n",
    "        # Write binds and non-binds to separate temporary files with unique names\n",
    "        if not df_binds.empty:\n",
    "            binds_file = f\"{output_dir}/{protein}_binds_chunk_{chunk_id}.parquet\"\n",
    "            df_binds.to_parquet(binds_file, engine='pyarrow', compression='snappy')\n",
    "\n",
    "        if not df_non_binds.empty:\n",
    "            non_binds_file = f\"{output_dir}/{protein}_non_binds_chunk_{chunk_id}.parquet\"\n",
    "            df_non_binds.to_parquet(non_binds_file, engine='pyarrow', compression='snappy')\n",
    "    logger.info(f\"Finished processing chunk {chunk_id}\")\n",
    "\n",
    "def process_bind_chunk(bind_chunk, non_binds_files, output_dir, protein):\n",
    "    result_rows = []\n",
    "    non_binds_file_iter = iter(non_binds_files)\n",
    "    current_non_binds = pd.read_parquet(next(non_binds_file_iter), engine='pyarrow')\n",
    "    non_binds_row_iter = iter(current_non_binds.iterrows())\n",
    "\n",
    "    for _, bind_row in bind_chunk.iterrows():\n",
    "        if not check_smiles(bind_row['molecule_smiles']):\n",
    "            continue\n",
    "\n",
    "        non_binds = []\n",
    "        while len(non_binds) < 2:\n",
    "            try:\n",
    "                _, non_bind = next(non_binds_row_iter)\n",
    "            except StopIteration:\n",
    "                try:\n",
    "                    current_non_binds = pd.read_parquet(next(non_binds_file_iter), engine='pyarrow')\n",
    "                    non_binds_row_iter = iter(current_non_binds.iterrows())\n",
    "                    continue\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "            if check_smiles(non_bind['molecule_smiles']):\n",
    "                non_binds.append(non_bind)\n",
    "\n",
    "        if len(non_binds) < 2:\n",
    "            break\n",
    "\n",
    "        result_rows.append({\n",
    "            'id': f\"{bind_row['id']}_{non_binds[0]['id']}_{non_binds[1]['id']}\",\n",
    "            'smiles_binds': bind_row['molecule_smiles'],\n",
    "            'smiles_non_binds_1': non_binds[0]['molecule_smiles'],\n",
    "            'smiles_non_binds_2': non_binds[1]['molecule_smiles'],\n",
    "            'protein_name': bind_row['protein_name']\n",
    "        })\n",
    "\n",
    "    return result_rows\n",
    "\n",
    "def merge_and_sample_non_binds(output_dir, protein):\n",
    "    \"\"\"Merge temporary Parquet files for binds/non-binds and sample two non-binds for each bind.\"\"\"\n",
    "    logger.info(f\"Starting merge and sample for protein {protein}\")\n",
    "    binds_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith(f\"{protein}_binds_chunk_\")]\n",
    "    non_binds_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith(f\"{protein}_non_binds_chunk_\")]\n",
    "\n",
    "    if not binds_files or not non_binds_files:\n",
    "        logger.warning(f\"No files found for protein {protein}\")\n",
    "        return None\n",
    "\n",
    "    binds_data = pd.concat(pd.read_parquet(file, engine='pyarrow') for file in binds_files)\n",
    "    max_binds = len(binds_data) * 3\n",
    "\n",
    "    # Split binds_data into chunks\n",
    "    chunk_size = 1000  # Adjust this value based on your system's capabilities\n",
    "    binds_chunks = [binds_data[i:i+chunk_size] for i in range(0, len(binds_data), chunk_size)]\n",
    "\n",
    "    output_file = f\"{output_dir}/{protein}_cleaned.parquet\"\n",
    "\n",
    "    # Define the schema for the output file\n",
    "    schema = pa.schema([\n",
    "        ('id', pa.string()),\n",
    "        ('smiles_binds', pa.string()),\n",
    "        ('smiles_non_binds_1', pa.string()),\n",
    "        ('smiles_non_binds_2', pa.string()),\n",
    "        ('protein_name', pa.string())\n",
    "    ])\n",
    "\n",
    "    # Create a ParquetWriter\n",
    "    with pq.ParquetWriter(output_file, schema) as writer:\n",
    "        logger.info(f\"Processing {max_binds} binds for protein {protein}\")\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for bind_chunk in binds_chunks:\n",
    "                # Create a random subset of non_binds_files for each chunk\n",
    "                non_binds_files_subset = random.sample(non_binds_files, min(len(non_binds_files), 10))\n",
    "                future = executor.submit(process_bind_chunk, bind_chunk, non_binds_files_subset, output_dir, protein)\n",
    "                futures.append(future)\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=f\"Processing {protein}\"):\n",
    "                chunk_results = future.result()\n",
    "                # Write chunk results to the Parquet file\n",
    "                if chunk_results:\n",
    "                    table = pa.Table.from_pylist(chunk_results, schema=schema)\n",
    "                    writer.write_table(table)\n",
    "\n",
    "    logger.info(f\"Finished processing for protein {protein}. Cleaning up intermediate files.\")\n",
    "    # Clean up the intermediate files\n",
    "    for f in binds_files + non_binds_files:\n",
    "        os.remove(f)\n",
    "\n",
    "def process_large_parquet(input_file, output_dir):\n",
    "    \"\"\"Process the large dataset in parallel chunks and save to temporary files.\"\"\"\n",
    "    logger.info(f\"Starting to process large parquet file: {input_file}\")\n",
    "    # Open parquet file\n",
    "    parquet_file = pq.ParquetFile(input_file)\n",
    "    \n",
    "    # Process each row group (chunk) in parallel\n",
    "    tasks = []\n",
    "    with tqdm(total=parquet_file.num_row_groups, dynamic_ncols=True) as pbar:\n",
    "        for i in range(parquet_file.num_row_groups):\n",
    "            # Read a chunk of the parquet file\n",
    "            chunk = parquet_file.read_row_group(i).to_pandas()\n",
    "\n",
    "            # Append the task to the list\n",
    "            tasks.append(delayed(process_chunk)(chunk, output_dir, i))\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    logger.info(\"Executing parallel tasks for chunk processing\")\n",
    "    # Execute the tasks in parallel\n",
    "    Parallel(n_jobs=-1, timeout=600)(tasks)\n",
    "\n",
    "    logger.info(\"Starting merge and sample for each protein\")\n",
    "    # Merge and sample non-binds for each protein\n",
    "    for protein in proteins:\n",
    "        merge_and_sample_non_binds(output_dir, protein)\n",
    "\n",
    "    logger.info(\"Merging final cleaned data for all proteins\")\n",
    "    # Merge the final cleaned data for all proteins into one file\n",
    "    cleaned_files = [f\"{output_dir}/{protein}_cleaned.parquet\" for protein in proteins]\n",
    "    df_final = pd.concat([pd.read_parquet(f) for f in cleaned_files])\n",
    "    df_final.to_parquet(f\"{output_dir}/cleaned_train.parquet\", engine='pyarrow', compression='snappy')\n",
    "\n",
    "    logger.info(\"Cleaning up intermediate files\")\n",
    "    # Clean up intermediate files\n",
    "    for f in cleaned_files:\n",
    "        os.remove(f)\n",
    "\n",
    "    logger.info(\"Processing complete\")\n",
    "\n",
    "# Define the input file and output directory\n",
    "input_parquet = \"train.parquet\"\n",
    "output_directory = \".\"\n",
    "\n",
    "# Run the parallel processing and merging\n",
    "process_large_parquet(input_parquet, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31eaed-0c37-4729-82ca-2d6de2068cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
