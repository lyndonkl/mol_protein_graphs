{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc08147-e0db-4a7e-b31a-630db155809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles', 'molecule_smiles', 'protein_name', 'binds']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('train.parquet')\n",
    "print(parquet_file.schema.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b78321-f080-459e-a494-2c842399c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0b1ae-6b2f-471f-bf90-23bf1f03ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of training rows: {total_rows}\")\n",
    "\n",
    "# Total number of rows in test dataset\n",
    "test_total_rows = test_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of test rows: {test_total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d0a601-fee4-4858-8c58-194dc99d47ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 282/282 [00:33<00:00,  8.30it/s]\n",
      "/Users/kushaldsouza/miniconda3/envs/pyg/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Run the parallel processing and merging\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m process_large_parquet(input_parquet, output_directory)\n",
      "Cell \u001b[0;32mIn[2], line 105\u001b[0m, in \u001b[0;36mprocess_large_parquet\u001b[0;34m(input_file, output_dir, batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Merge and sample non-binds for each protein\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m protein \u001b[38;5;129;01min\u001b[39;00m proteins:\n\u001b[0;32m--> 105\u001b[0m     merge_and_sample_non_binds(output_dir, protein)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Merge the final cleaned data for all proteins into one file\u001b[39;00m\n\u001b[1;32m    108\u001b[0m cleaned_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotein\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cleaned.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m protein \u001b[38;5;129;01min\u001b[39;00m proteins]\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36mmerge_and_sample_non_binds\u001b[0;34m(output_dir, protein)\u001b[0m\n\u001b[1;32m     48\u001b[0m result_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m non_binds_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, bind_row \u001b[38;5;129;01min\u001b[39;00m df_binds\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m non_binds_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_non_binds):\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Stop if we run out of non-binds\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/pandas/core/frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m klass(v, index\u001b[38;5;241m=\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mk)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/pandas/core/series.py:593\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    590\u001b[0m         data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n\u001b[1;32m    592\u001b[0m NDFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_axis(\u001b[38;5;241m0\u001b[39m, index)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/pandas/core/generic.py:6320\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6317\u001b[0m \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n\u001b[1;32m   6318\u001b[0m \u001b[38;5;66;03m# (note that this matches __getattr__, above).\u001b[39;00m\n\u001b[1;32m   6319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set:\n\u001b[0;32m-> 6320\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m   6321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata:\n\u001b[1;32m   6322\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.12/site-packages/pandas/core/series.py:784\u001b[0m, in \u001b[0;36mSeries.name\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    Return the name of the Series.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m    'Even Numbers'\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n\u001b[0;32m--> 784\u001b[0m \u001b[38;5;129m@name\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     validate_all_hashable(value, error_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 1_000_000  # Process in chunks to handle large data efficiently\n",
    "proteins = ['HSA', 'sEH', 'BRD4']\n",
    "\n",
    "def process_chunk(df_chunk, output_dir, chunk_id):\n",
    "    \"\"\"Process a chunk of the dataframe and save binds/non-binds to separate temporary files for each protein.\"\"\"\n",
    "    for protein in proteins:\n",
    "        # Filter based on protein\n",
    "        df_protein = df_chunk[df_chunk['protein_name'] == protein]\n",
    "\n",
    "        # Separate into binds and non-binds\n",
    "        df_binds = df_protein[df_protein['binds'] == 1]\n",
    "        df_non_binds = df_protein[df_protein['binds'] == 0]\n",
    "\n",
    "        # Write binds and non-binds to separate temporary files with unique names\n",
    "        if not df_binds.empty:\n",
    "            binds_file = f\"{output_dir}/{protein}_binds_chunk_{chunk_id}.parquet\"\n",
    "            df_binds.to_parquet(binds_file, engine='pyarrow', compression='snappy')\n",
    "\n",
    "        if not df_non_binds.empty:\n",
    "            non_binds_file = f\"{output_dir}/{protein}_non_binds_chunk_{chunk_id}.parquet\"\n",
    "            df_non_binds.to_parquet(non_binds_file, engine='pyarrow', compression='snappy')\n",
    "\n",
    "def merge_and_sample_non_binds(output_dir, protein):\n",
    "    \"\"\"Merge temporary Parquet files for binds/non-binds and sample two non-binds for each bind.\"\"\"\n",
    "    binds_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith(f\"{protein}_binds_chunk_\")]\n",
    "    non_binds_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith(f\"{protein}_non_binds_chunk_\")]\n",
    "\n",
    "    if not binds_files or not non_binds_files:\n",
    "        return None\n",
    "\n",
    "    # Read and merge binds\n",
    "    df_binds = pd.concat([pd.read_parquet(f) for f in binds_files])\n",
    "\n",
    "    # Read and merge non-binds\n",
    "    df_non_binds = pd.concat([pd.read_parquet(f) for f in non_binds_files])\n",
    "\n",
    "    # Shuffle the non-binds to get a random selection\n",
    "    df_non_binds = df_non_binds.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    result_rows = []\n",
    "    non_binds_index = 0\n",
    "    for _, bind_row in df_binds.iterrows():\n",
    "        if non_binds_index + 1 >= len(df_non_binds):\n",
    "            break  # Stop if we run out of non-binds\n",
    "\n",
    "        non_bind_1 = df_non_binds.iloc[non_binds_index]\n",
    "        non_bind_2 = df_non_binds.iloc[non_binds_index + 1]\n",
    "\n",
    "        # Append a row to the result\n",
    "        result_rows.append({\n",
    "            'id': bind_row['id'],\n",
    "            'smiles_binds': bind_row['molecule_smiles'],\n",
    "            'smiles_non_binds_1': non_bind_1['molecule_smiles'],\n",
    "            'smiles_non_binds_2': non_bind_2['molecule_smiles'],\n",
    "            'protein_name': bind_row['protein_name']\n",
    "        })\n",
    "\n",
    "        # Increment the non-binds index by 2 for each bind\n",
    "        if non_binds_index + 2 >= len(df_non_binds):\n",
    "            break \n",
    "        non_binds_index += 2\n",
    "\n",
    "    # Create a DataFrame with the triplets\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "\n",
    "    # Write to a single file for this protein\n",
    "    output_file = f\"{output_dir}/{protein}_cleaned.parquet\"\n",
    "    result_df.to_parquet(output_file, engine='pyarrow', compression='snappy')\n",
    "\n",
    "    # Clean up the intermediate files\n",
    "    for f in binds_files + non_binds_files:\n",
    "        os.remove(f)\n",
    "\n",
    "def process_large_parquet(input_file, output_dir, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Process the large dataset in parallel chunks and save to temporary files.\"\"\"\n",
    "    # Open parquet file\n",
    "    parquet_file = pq.ParquetFile(input_file)\n",
    "    \n",
    "    # Process each row group (chunk) in parallel\n",
    "    tasks = []\n",
    "    with tqdm(total=parquet_file.num_row_groups, dynamic_ncols=True) as pbar:  # Adjust tqdm for parallel processing\n",
    "        for i in range(parquet_file.num_row_groups):\n",
    "            # Read a chunk of the parquet file\n",
    "            df_chunk = parquet_file.read_row_group(i).to_pandas()\n",
    "\n",
    "            # Append the task to the list\n",
    "            tasks.append(delayed(process_chunk)(df_chunk, output_dir, i))\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Execute the tasks in parallel\n",
    "    Parallel(n_jobs=-1)(tasks)\n",
    "\n",
    "    # Merge and sample non-binds for each protein\n",
    "    for protein in proteins:\n",
    "        merge_and_sample_non_binds(output_dir, protein)\n",
    "\n",
    "    # Merge the final cleaned data for all proteins into one file\n",
    "    cleaned_files = [f\"{output_dir}/{protein}_cleaned.parquet\" for protein in proteins]\n",
    "    df_final = pd.concat([pd.read_parquet(f) for f in cleaned_files])\n",
    "    df_final.to_parquet(f\"{output_dir}/cleaned_train.parquet\", engine='pyarrow', compression='snappy')\n",
    "\n",
    "    # Clean up intermediate files\n",
    "    for f in cleaned_files:\n",
    "        os.remove(f)\n",
    "\n",
    "# Define the input file and output directory\n",
    "input_parquet = \"train.parquet\"\n",
    "output_directory = \".\"\n",
    "\n",
    "# Run the parallel processing and merging\n",
    "process_large_parquet(input_parquet, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b010c-993f-4c36-a96c-ca1de2f96181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
