{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc08147-e0db-4a7e-b31a-630db155809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('train.parquet')\n",
    "print(parquet_file.schema.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0b1ae-6b2f-471f-bf90-23bf1f03ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = dd.read_parquet('train.parquet')\n",
    "test_df = dd.read_parquet('test.parquet')\n",
    "\n",
    "# Total number of rows in train dataset\n",
    "total_rows = train_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of training rows: {total_rows}\")\n",
    "\n",
    "# Total number of rows in test dataset\n",
    "test_total_rows = test_df.map_partitions(len).compute().sum()\n",
    "print(f\"Total number of test rows: {test_total_rows}\")\n",
    "\n",
    "# Number of positive bindings\n",
    "num_positive_bindings = train_df['binds'].sum().compute()\n",
    "print(f\"Number of positive bindings: {num_positive_bindings}\")\n",
    "\n",
    "# Number of negative bindings\n",
    "num_negative_bindings = total_rows - num_positive_bindings\n",
    "print(f\"Number of negative bindings: {num_negative_bindings}\")\n",
    "\n",
    "# Percentage calculations\n",
    "percent_positive = (num_positive_bindings / total_rows) * 100\n",
    "percent_negative = (num_negative_bindings / total_rows) * 100\n",
    "print(f\"Percentage of positive bindings: {percent_positive:.2f}%\")\n",
    "print(f\"Percentage of negative bindings: {percent_negative:.2f}%\")\n",
    "\n",
    "# Total unique proteins in train dataset\n",
    "unique_proteins_train = train_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_train = len(unique_proteins_train)\n",
    "print(f\"Total unique proteins in train dataset: {total_unique_proteins_train}\")\n",
    "\n",
    "# Total unique proteins in test dataset\n",
    "unique_proteins_test = test_df['protein_name'].dropna().unique().compute()\n",
    "total_unique_proteins_test = len(unique_proteins_test)\n",
    "print(f\"Total unique proteins in test dataset: {total_unique_proteins_test}\")\n",
    "\n",
    "# Total unique proteins in both datasets\n",
    "unique_proteins_all = dd.concat([\n",
    "    train_df['protein_name'],\n",
    "    test_df['protein_name']\n",
    "]).dropna().unique().compute()\n",
    "total_unique_proteins_all = len(unique_proteins_all)\n",
    "print(f\"Total unique proteins in both datasets: {total_unique_proteins_all}\")\n",
    "print(f\"Unique proteins in both datasets: {unique_proteins_all.values}\")\n",
    "\n",
    "# Concatenate building block columns from both datasets\n",
    "train_building_blocks = dd.concat([\n",
    "    train_df['buildingblock1_smiles'],\n",
    "    train_df['buildingblock2_smiles'],\n",
    "    train_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "test_building_blocks = dd.concat([\n",
    "    test_df['buildingblock1_smiles'],\n",
    "    test_df['buildingblock2_smiles'],\n",
    "    test_df['buildingblock3_smiles']\n",
    "])\n",
    "\n",
    "all_building_blocks = dd.concat([train_building_blocks, test_building_blocks])\n",
    "\n",
    "# Compute unique building blocks\n",
    "unique_building_blocks = all_building_blocks.dropna().unique().compute()\n",
    "total_unique_building_blocks = len(unique_building_blocks)\n",
    "print(f\"Total unique building blocks (train and test): {total_unique_building_blocks}\")\n",
    "\n",
    "# Compute unique small molecules from train and test\n",
    "train_small_molecules = train_df['molecule_smiles'].dropna()\n",
    "test_small_molecules = test_df['molecule_smiles'].dropna()\n",
    "all_small_molecules = dd.concat([train_small_molecules, test_small_molecules])\n",
    "\n",
    "unique_small_molecules = all_small_molecules.unique().compute()\n",
    "total_unique_small_molecules = len(unique_small_molecules)\n",
    "print(f\"Total unique small molecules (train and test): {total_unique_small_molecules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6afec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 16:08:34,912 - INFO - Loading datasets...\n",
      "2024-10-26 16:08:35,819 - INFO - Train dataset shape: (1589770, 5)\n",
      "2024-10-26 16:08:35,819 - INFO - Test dataset shape: (1674896, 6)\n",
      "2024-10-26 16:08:35,819 - INFO - Collecting unique atom and edge types from train dataset...\n",
      "Collecting atom and edge types: 100%|██████████| 1589770/1589770 [03:44<00:00, 7081.42it/s] \n",
      "2024-10-26 16:12:22,574 - INFO - Collecting unique atom and edge types from test dataset...\n",
      "Collecting atom and edge types: 100%|██████████| 1674896/1674896 [02:14<00:00, 12464.28it/s]\n",
      "2024-10-26 16:14:38,551 - INFO - Unique node types in train dataset:\n",
      "2024-10-26 16:14:38,551 - INFO - ['B', 'Br', 'C', 'Cl', 'F', 'H', 'I', 'N', 'O', 'S', 'Si']\n",
      "2024-10-26 16:14:38,552 - INFO - Number of unique node types in train dataset: 11\n",
      "2024-10-26 16:14:38,552 - INFO - Number of unique edge types in train dataset: 37\n",
      "2024-10-26 16:14:38,552 - INFO - Unique node types in test dataset:\n",
      "2024-10-26 16:14:38,553 - INFO - ['B', 'Br', 'C', 'Cl', 'F', 'H', 'I', 'N', 'O', 'S', 'Si']\n",
      "2024-10-26 16:14:38,553 - INFO - Number of unique node types in test dataset: 11\n",
      "2024-10-26 16:14:38,553 - INFO - Number of unique edge types in test dataset: 37\n",
      "2024-10-26 16:14:38,554 - INFO - Combined unique node types:\n",
      "2024-10-26 16:14:38,554 - INFO - ['B', 'Br', 'C', 'Cl', 'F', 'H', 'I', 'N', 'O', 'S', 'Si']\n",
      "2024-10-26 16:14:38,554 - INFO - Number of combined unique node types: 11\n",
      "2024-10-26 16:14:38,554 - INFO - Number of combined unique edge types: 37\n",
      "2024-10-26 16:14:38,556 - INFO - Process completed. Results saved to unique_atom_and_edge_types.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from rdkit import Chem\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def process_row_for_types(row, is_train=False):\n",
    "    try:\n",
    "        if is_train:\n",
    "            smiles_list = [row['smiles_binds'], row['smiles_non_binds_1'], row['smiles_non_binds_2']]\n",
    "        else:\n",
    "            smiles_list = [row['molecule_smiles']]\n",
    "\n",
    "        all_atom_types = set()\n",
    "        all_edge_types = set()\n",
    "\n",
    "        for smiles in smiles_list:\n",
    "            if pd.isna(smiles):\n",
    "                continue  # Skip NaN values\n",
    "\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                logger.warning(f\"Invalid SMILES: {smiles}\")\n",
    "                continue  # Skip invalid SMILES\n",
    "\n",
    "            # Remove atoms and process the molecule\n",
    "            atoms_to_remove = [atom.GetIdx() for atom in mol.GetAtoms() if atom.GetSymbol() == 'Dy']\n",
    "            mol = Chem.EditableMol(mol)\n",
    "            for idx in sorted(atoms_to_remove, reverse=True):\n",
    "                mol.RemoveAtom(idx)\n",
    "            mol = mol.GetMol()\n",
    "\n",
    "            mol = Chem.AddHs(mol)\n",
    "\n",
    "            # Collect atom and edge types\n",
    "            atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "            all_atom_types.update(atom_types)\n",
    "\n",
    "            for bond in mol.GetBonds():\n",
    "                i = bond.GetBeginAtomIdx()\n",
    "                j = bond.GetEndAtomIdx()\n",
    "                atype_i = atom_types[i]\n",
    "                atype_j = atom_types[j]\n",
    "                all_edge_types.add((atype_i, 'bond', atype_j))\n",
    "                all_edge_types.add((atype_j, 'bond', atype_i))\n",
    "\n",
    "        return all_atom_types, all_edge_types\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing row: {e}\")\n",
    "        return set(), set()\n",
    "\n",
    "def collect_unique_atom_and_edge_types(df, is_train=False):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_row_for_types)(row, is_train) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting atom and edge types\")\n",
    "    )\n",
    "\n",
    "    molecule_node_types = set()\n",
    "    molecule_edge_types = set()\n",
    "    for node_types, edge_types in results:\n",
    "        molecule_node_types.update(node_types)\n",
    "        molecule_edge_types.update(edge_types)\n",
    "\n",
    "    return molecule_node_types, molecule_edge_types\n",
    "\n",
    "# Load the cleaned data\n",
    "logger.info(\"Loading datasets...\")\n",
    "cleaned_train_df = pd.read_parquet('./cleaned_train_unique.parquet')\n",
    "cleaned_test_df = pd.read_parquet('./test.parquet')\n",
    "\n",
    "logger.info(f\"Train dataset shape: {cleaned_train_df.shape}\")\n",
    "logger.info(f\"Test dataset shape: {cleaned_test_df.shape}\")\n",
    "\n",
    "# Collect unique atom and edge types from both train and test datasets\n",
    "logger.info(\"Collecting unique atom and edge types from train dataset...\")\n",
    "train_node_types, train_edge_types = collect_unique_atom_and_edge_types(cleaned_train_df, is_train=True)\n",
    "\n",
    "logger.info(\"Collecting unique atom and edge types from test dataset...\")\n",
    "test_node_types, test_edge_types = collect_unique_atom_and_edge_types(cleaned_test_df, is_train=False)\n",
    "\n",
    "# Log unique types for train dataset\n",
    "logger.info(\"Unique node types in train dataset:\")\n",
    "logger.info(sorted(list(train_node_types)))\n",
    "logger.info(f\"Number of unique node types in train dataset: {len(train_node_types)}\")\n",
    "logger.info(f\"Number of unique edge types in train dataset: {len(train_edge_types)}\")\n",
    "\n",
    "# Log unique types for test dataset\n",
    "logger.info(\"Unique node types in test dataset:\")\n",
    "logger.info(sorted(list(test_node_types)))\n",
    "logger.info(f\"Number of unique node types in test dataset: {len(test_node_types)}\")\n",
    "logger.info(f\"Number of unique edge types in test dataset: {len(test_edge_types)}\")\n",
    "\n",
    "# Combine the unique types from both datasets\n",
    "combined_node_types = train_node_types.union(test_node_types)\n",
    "combined_edge_types = train_edge_types.union(test_edge_types)\n",
    "\n",
    "# Save the combined unique atom and edge types to a JSON file\n",
    "unique_types = {\n",
    "    'molecule_node_types': sorted(list(combined_node_types)),\n",
    "    'molecule_edge_types': sorted([list(edge_type) for edge_type in combined_edge_types])\n",
    "}\n",
    "\n",
    "logger.info(\"Combined unique node types:\")\n",
    "logger.info(unique_types['molecule_node_types'])\n",
    "logger.info(f\"Number of combined unique node types: {len(unique_types['molecule_node_types'])}\")\n",
    "logger.info(f\"Number of combined unique edge types: {len(unique_types['molecule_edge_types'])}\")\n",
    "\n",
    "with open('unique_atom_and_edge_types.json', 'w') as f:\n",
    "    json.dump(unique_types, f, indent=4)\n",
    "\n",
    "logger.info(\"Process completed. Results saved to unique_atom_and_edge_types.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
